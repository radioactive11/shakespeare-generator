{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BOwsuGQQY9OL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "PRnDnCW-Z7qv",
    "outputId": "098790d0-d48d-4494-b13e-525ceb5073c6"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "\n",
    "data = open('sonnets.txt').read()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "# create input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for line in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_sequence = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_sequence)\n",
    "\n",
    "\n",
    "# pad sequences \n",
    "max_sequence_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "\n",
    "# create predictors and label\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w9vH8Y59ajYL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 100)           321100    \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 10, 300)           301200    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 10, 300)           0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 100)               160400    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1605)              162105    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3211)              5156866   \n",
      "=================================================================\n",
      "Total params: 6,101,671\n",
      "Trainable params: 6,101,671\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(Bidirectional(LSTM(150, return_sequences = True)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AIg2f1HBxqof"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15462 samples\n",
      "Epoch 1/100\n",
      "15462/15462 [==============================] - 48s 3ms/sample - loss: 6.9185 - accuracy: 0.0212\n",
      "Epoch 2/100\n",
      "15462/15462 [==============================] - 43s 3ms/sample - loss: 6.5023 - accuracy: 0.0233\n",
      "Epoch 3/100\n",
      "15462/15462 [==============================] - 43s 3ms/sample - loss: 6.4076 - accuracy: 0.0239\n",
      "Epoch 4/100\n",
      "15462/15462 [==============================] - 44s 3ms/sample - loss: 6.2969 - accuracy: 0.0276\n",
      "Epoch 5/100\n",
      "15462/15462 [==============================] - 44s 3ms/sample - loss: 6.2023 - accuracy: 0.0334\n",
      "Epoch 6/100\n",
      "15462/15462 [==============================] - 44s 3ms/sample - loss: 6.1174 - accuracy: 0.0376\n",
      "Epoch 7/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 6.0408 - accuracy: 0.0396s - loss: 6\n",
      "Epoch 8/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 5.9632 - accuracy: 0.0429\n",
      "Epoch 9/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 5.8685 - accuracy: 0.0501\n",
      "Epoch 10/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 5.7636 - accuracy: 0.0561\n",
      "Epoch 11/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 5.6569 - accuracy: 0.0614\n",
      "Epoch 12/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 5.5449 - accuracy: 0.0661\n",
      "Epoch 13/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 5.4374 - accuracy: 0.0733\n",
      "Epoch 14/100\n",
      "15462/15462 [==============================] - 47s 3ms/sample - loss: 5.3225 - accuracy: 0.0792\n",
      "Epoch 15/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 5.2149 - accuracy: 0.0842\n",
      "Epoch 16/100\n",
      "15462/15462 [==============================] - 43s 3ms/sample - loss: 5.0975 - accuracy: 0.0921\n",
      "Epoch 17/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 4.9921 - accuracy: 0.0998s - loss: 4.9911 - ac\n",
      "Epoch 18/100\n",
      "15462/15462 [==============================] - 47s 3ms/sample - loss: 4.8883 - accuracy: 0.1069\n",
      "Epoch 19/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 4.7808 - accuracy: 0.1194\n",
      "Epoch 20/100\n",
      "15462/15462 [==============================] - 44s 3ms/sample - loss: 4.6687 - accuracy: 0.1262\n",
      "Epoch 21/100\n",
      "15462/15462 [==============================] - 44s 3ms/sample - loss: 4.5663 - accuracy: 0.1338\n",
      "Epoch 22/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 4.4661 - accuracy: 0.1440\n",
      "Epoch 23/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 4.3557 - accuracy: 0.1578s - loss: 4.354\n",
      "Epoch 24/100\n",
      "15462/15462 [==============================] - 49s 3ms/sample - loss: 4.2542 - accuracy: 0.1698\n",
      "Epoch 25/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 4.1412 - accuracy: 0.1820\n",
      "Epoch 26/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 4.0435 - accuracy: 0.1948\n",
      "Epoch 27/100\n",
      "15462/15462 [==============================] - 55s 4ms/sample - loss: 3.9457 - accuracy: 0.2084\n",
      "Epoch 28/100\n",
      "15462/15462 [==============================] - 54s 3ms/sample - loss: 3.8439 - accuracy: 0.2247\n",
      "Epoch 29/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 3.7344 - accuracy: 0.2451\n",
      "Epoch 30/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 3.6514 - accuracy: 0.2636\n",
      "Epoch 31/100\n",
      "15462/15462 [==============================] - 63s 4ms/sample - loss: 3.5592 - accuracy: 0.2815\n",
      "Epoch 32/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 3.4626 - accuracy: 0.3005\n",
      "Epoch 33/100\n",
      "15462/15462 [==============================] - 54s 3ms/sample - loss: 3.3761 - accuracy: 0.3197\n",
      "Epoch 34/100\n",
      "15462/15462 [==============================] - 51s 3ms/sample - loss: 3.2883 - accuracy: 0.3434\n",
      "Epoch 35/100\n",
      "15462/15462 [==============================] - 60s 4ms/sample - loss: 3.2123 - accuracy: 0.3595\n",
      "Epoch 36/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 3.1288 - accuracy: 0.3792s - los\n",
      "Epoch 37/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 3.0503 - accuracy: 0.3931\n",
      "Epoch 38/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 2.9828 - accuracy: 0.4140\n",
      "Epoch 39/100\n",
      "15462/15462 [==============================] - 51s 3ms/sample - loss: 2.9126 - accuracy: 0.4280s - loss: 2.9104 - accura - ETA: 1s - loss: 2\n",
      "Epoch 40/100\n",
      "15462/15462 [==============================] - 55s 4ms/sample - loss: 2.8464 - accuracy: 0.4464\n",
      "Epoch 41/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 2.7829 - accuracy: 0.4553\n",
      "Epoch 42/100\n",
      "15462/15462 [==============================] - 62s 4ms/sample - loss: 2.7154 - accuracy: 0.4704\n",
      "Epoch 43/100\n",
      "15462/15462 [==============================] - 54s 3ms/sample - loss: 2.6596 - accuracy: 0.4862\n",
      "Epoch 44/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 2.5881 - accuracy: 0.5007\n",
      "Epoch 45/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 2.5330 - accuracy: 0.5169\n",
      "Epoch 46/100\n",
      "15462/15462 [==============================] - 54s 3ms/sample - loss: 2.4896 - accuracy: 0.5240\n",
      "Epoch 47/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 2.4275 - accuracy: 0.5396\n",
      "Epoch 48/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 2.3771 - accuracy: 0.5512\n",
      "Epoch 49/100\n",
      "15462/15462 [==============================] - 48s 3ms/sample - loss: 2.3368 - accuracy: 0.5634\n",
      "Epoch 50/100\n",
      "15462/15462 [==============================] - 47s 3ms/sample - loss: 2.2852 - accuracy: 0.5739\n",
      "Epoch 51/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 2.2289 - accuracy: 0.5845\n",
      "Epoch 52/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 2.2096 - accuracy: 0.5850\n",
      "Epoch 53/100\n",
      "15462/15462 [==============================] - 57s 4ms/sample - loss: 2.1729 - accuracy: 0.5954\n",
      "Epoch 54/100\n",
      "15462/15462 [==============================] - 58s 4ms/sample - loss: 2.1149 - accuracy: 0.6119\n",
      "Epoch 55/100\n",
      "15462/15462 [==============================] - 54s 3ms/sample - loss: 2.0778 - accuracy: 0.6178\n",
      "Epoch 56/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 2.0238 - accuracy: 0.6317\n",
      "Epoch 57/100\n",
      "15462/15462 [==============================] - 61s 4ms/sample - loss: 1.9920 - accuracy: 0.6433\n",
      "Epoch 58/100\n",
      "15462/15462 [==============================] - 55s 4ms/sample - loss: 1.9557 - accuracy: 0.6489\n",
      "Epoch 59/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 1.9197 - accuracy: 0.6553s - loss:\n",
      "Epoch 60/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 1.8893 - accuracy: 0.6619\n",
      "Epoch 61/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 1.8666 - accuracy: 0.6674\n",
      "Epoch 62/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 1.8327 - accuracy: 0.6714\n",
      "Epoch 63/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 1.8001 - accuracy: 0.6800\n",
      "Epoch 64/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 1.7609 - accuracy: 0.6879\n",
      "Epoch 65/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 1.7282 - accuracy: 0.6971\n",
      "Epoch 66/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 1.7032 - accuracy: 0.7021\n",
      "Epoch 67/100\n",
      "15462/15462 [==============================] - 51s 3ms/sample - loss: 1.6805 - accuracy: 0.7069\n",
      "Epoch 68/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 1.6521 - accuracy: 0.7097\n",
      "Epoch 69/100\n",
      "15462/15462 [==============================] - 54s 3ms/sample - loss: 1.6272 - accuracy: 0.7182\n",
      "Epoch 70/100\n",
      "15462/15462 [==============================] - 51s 3ms/sample - loss: 1.6176 - accuracy: 0.7160\n",
      "Epoch 71/100\n",
      "15462/15462 [==============================] - 51s 3ms/sample - loss: 1.5884 - accuracy: 0.7251\n",
      "Epoch 72/100\n",
      "15462/15462 [==============================] - 50s 3ms/sample - loss: 1.5544 - accuracy: 0.7308\n",
      "Epoch 73/100\n",
      "15462/15462 [==============================] - 50s 3ms/sample - loss: 1.5430 - accuracy: 0.7324\n",
      "Epoch 74/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 1.5204 - accuracy: 0.7406s - loss: 1.5179 - accuracy: 0.74 - ETA: 0s - loss: 1.5195 - accuracy: 0.\n",
      "Epoch 75/100\n",
      "15462/15462 [==============================] - 56s 4ms/sample - loss: 1.4954 - accuracy: 0.7419s - loss: 1.4933 - \n",
      "Epoch 76/100\n",
      "15462/15462 [==============================] - 52s 3ms/sample - loss: 1.4852 - accuracy: 0.7423\n",
      "Epoch 77/100\n",
      "15462/15462 [==============================] - 50s 3ms/sample - loss: 1.4638 - accuracy: 0.7488\n",
      "Epoch 78/100\n",
      "15462/15462 [==============================] - 49s 3ms/sample - loss: 1.4243 - accuracy: 0.7573\n",
      "Epoch 79/100\n",
      "15462/15462 [==============================] - 50s 3ms/sample - loss: 1.4094 - accuracy: 0.7589s - los\n",
      "Epoch 80/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 1.4011 - accuracy: 0.7609\n",
      "Epoch 81/100\n",
      "15462/15462 [==============================] - 54s 4ms/sample - loss: 1.3855 - accuracy: 0.7623\n",
      "Epoch 82/100\n",
      "15462/15462 [==============================] - 53s 3ms/sample - loss: 1.3767 - accuracy: 0.7668\n",
      "Epoch 83/100\n",
      "15462/15462 [==============================] - 51s 3ms/sample - loss: 1.3451 - accuracy: 0.7701\n",
      "Epoch 84/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 1.3427 - accuracy: 0.7729s - loss: 1.3420 - accu\n",
      "Epoch 85/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 1.3167 - accuracy: 0.7777\n",
      "Epoch 86/100\n",
      "15462/15462 [==============================] - 45s 3ms/sample - loss: 1.3063 - accuracy: 0.7751\n",
      "Epoch 87/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 1.2880 - accuracy: 0.7820\n",
      "Epoch 88/100\n",
      "15462/15462 [==============================] - 47s 3ms/sample - loss: 1.2950 - accuracy: 0.7745\n",
      "Epoch 89/100\n",
      "15462/15462 [==============================] - 46s 3ms/sample - loss: 1.2795 - accuracy: 0.7786\n",
      "Epoch 90/100\n",
      "15462/15462 [==============================] - 40s 3ms/sample - loss: 1.2563 - accuracy: 0.7845\n",
      "Epoch 91/100\n",
      "15462/15462 [==============================] - 41s 3ms/sample - loss: 1.2324 - accuracy: 0.7886\n",
      "Epoch 92/100\n",
      "15462/15462 [==============================] - 41s 3ms/sample - loss: 1.2148 - accuracy: 0.7946\n",
      "Epoch 93/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 1.2092 - accuracy: 0.7923\n",
      "Epoch 94/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 1.2078 - accuracy: 0.7921\n",
      "Epoch 95/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 1.2033 - accuracy: 0.7944\n",
      "Epoch 96/100\n",
      "15462/15462 [==============================] - 41s 3ms/sample - loss: 1.1869 - accuracy: 0.7947\n",
      "Epoch 97/100\n",
      "15462/15462 [==============================] - 43s 3ms/sample - loss: 1.1844 - accuracy: 0.7950\n",
      "Epoch 98/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 1.1706 - accuracy: 0.7969\n",
      "Epoch 99/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 1.1555 - accuracy: 0.7985\n",
      "Epoch 100/100\n",
      "15462/15462 [==============================] - 42s 3ms/sample - loss: 1.1497 - accuracy: 0.8036s - loss: 1.147\n"
     ]
    }
   ],
   "source": [
    " history = model.fit(predictors, label, epochs=100, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1fXTEO3GJ282"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'b', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'b', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6Vc6PHgxa6Hm"
   },
   "outputs": [],
   "source": [
    "seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n",
    "next_words = 100\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\toutput_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\toutput_word = word\n",
    "\t\t\tbreak\n",
    "\tseed_text += \" \" + output_word\n",
    "print(seed_text)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "NLP_Week4_Exercise_Shakespeare_Answer.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
